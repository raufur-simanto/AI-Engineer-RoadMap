{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Day 1 Morning: Attention Mechanism from Scratch\n",
    "\n",
    "## Learning Objectives\n",
    "- ‚úÖ Understand the attention mechanism deeply\n",
    "- ‚úÖ Implement scaled dot-product attention in PyTorch\n",
    "- ‚úÖ Visualize attention weights\n",
    "- ‚úÖ Understand Q, K, V matrices\n",
    "\n",
    "## Key Formula\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- **Q** (Query): What we're looking for\n",
    "- **K** (Key): What we're looking at\n",
    "- **V** (Value): What we actually get\n",
    "- **d_k**: Dimension of keys (for scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (Colab already has most of these)\n",
    "!pip install -q torch matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Implement Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \n",
    "    Formula: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "    \"\"\"\n",
    "    def __init__(self, d_k):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: Query (batch_size, seq_len, d_k)\n",
    "            K: Key (batch_size, seq_len, d_k)\n",
    "            V: Value (batch_size, seq_len, d_v)\n",
    "            mask: Optional mask\n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, d_v)\n",
    "            attention_weights: (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # Step 1: Compute attention scores (QK^T)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        \n",
    "        # Step 2: Scale by sqrt(d_k)\n",
    "        scores = scores / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        \n",
    "        # Step 3: Apply mask (optional)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Step 4: Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 5: Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"‚úÖ Attention class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Visualization Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, tokens=None, title=\"Attention Weights\"):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    weights = attention_weights.detach().cpu().numpy()\n",
    "    \n",
    "    sns.heatmap(weights, annot=True, fmt='.2f', cmap='viridis',\n",
    "                xticklabels=tokens if tokens else range(weights.shape[1]),\n",
    "                yticklabels=tokens if tokens else range(weights.shape[0]),\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    \n",
    "    plt.xlabel('Key Position', fontsize=12)\n",
    "    plt.ylabel('Query Position', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Example 1: Simple Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Example 1: Simple Attention Mechanism\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 1\n",
    "seq_len = 4\n",
    "d_k = 8  # Dimension of queries and keys\n",
    "d_v = 8  # Dimension of values\n",
    "\n",
    "# Create random Q, K, V matrices\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "print(f\"\\nInput shapes:\")\n",
    "print(f\"  Q (Query): {Q.shape}\")\n",
    "print(f\"  K (Key): {K.shape}\")\n",
    "print(f\"  V (Value): {V.shape}\")\n",
    "\n",
    "# Apply attention\n",
    "attention = ScaledDotProductAttention(d_k)\n",
    "output, attention_weights = attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "print(f\"  Attention weights: {attention_weights.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Attention weights (should sum to 1 for each query):\")\n",
    "print(attention_weights[0])\n",
    "print(f\"\\nSum per row: {attention_weights[0].sum(dim=-1)}\")\n",
    "\n",
    "# Visualize\n",
    "tokens = ['The', 'cat', 'sat', 'down']\n",
    "visualize_attention(attention_weights[0], tokens, \"Simple Attention Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Example 2: Self-Attention\n",
    "\n",
    "In self-attention, Q, K, V all come from the same input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Example 2: Self-Attention\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "\n",
    "# Simulate input embeddings\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Linear projections to get Q, K, V from same input\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "Q = W_q(X)\n",
    "K = W_k(X)\n",
    "V = W_v(X)\n",
    "\n",
    "print(f\"\\nInput X shape: {X.shape}\")\n",
    "print(f\"Q, K, V shapes: {Q.shape}\")\n",
    "\n",
    "# Apply self-attention\n",
    "attention = ScaledDotProductAttention(d_model)\n",
    "output, attention_weights = attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nSelf-attention output shape: {output.shape}\")\n",
    "\n",
    "# Visualize\n",
    "tokens = ['I', 'love', 'machine', 'learning', '!']\n",
    "visualize_attention(attention_weights[0], tokens, \"Self-Attention Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Example 3: Masked (Causal) Attention\n",
    "\n",
    "Used in GPT-like models - each position can only attend to itself and previous positions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Example 3: Masked (Causal) Attention\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "d_k = 8\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "# Create causal mask (lower triangular)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "\n",
    "print(f\"\\nCausal mask (1 = can attend, 0 = cannot attend):\")\n",
    "print(mask[0])\n",
    "\n",
    "# Apply masked attention\n",
    "attention = ScaledDotProductAttention(d_k)\n",
    "output, attention_weights = attention(Q, K, V, mask)\n",
    "\n",
    "print(f\"\\nüìä Masked attention weights:\")\n",
    "print(attention_weights[0])\n",
    "print(\"\\nüí° Notice: Each position only attends to itself and previous positions!\")\n",
    "\n",
    "# Visualize\n",
    "tokens = ['The', 'cat', 'is', 'very', 'cute']\n",
    "visualize_attention(attention_weights[0], tokens, \"Causal (Masked) Attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Key Takeaways\n",
    "\n",
    "1. **Attention allows the model to focus on relevant parts of the input**\n",
    "   - Each query attends to all keys with different weights\n",
    "   - Weights sum to 1 (softmax normalization)\n",
    "\n",
    "2. **Scaling by ‚àöd_k prevents softmax saturation**\n",
    "   - Without scaling, dot products can become very large\n",
    "   - Large values ‚Üí softmax becomes too peaked ‚Üí gradients vanish\n",
    "\n",
    "3. **Masks enable different attention patterns**\n",
    "   - Causal mask: For autoregressive models (GPT)\n",
    "   - Padding mask: Ignore padding tokens\n",
    "\n",
    "4. **Attention weights are interpretable**\n",
    "   - We can visualize what the model focuses on\n",
    "   - Useful for debugging and understanding model behavior\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "Continue to **Day 1 Afternoon: Multi-Head Attention** to learn:\n",
    "- Why use multiple attention heads?\n",
    "- How do heads specialize?\n",
    "- Implementing multi-head attention from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Experiment!\n",
    "\n",
    "Try modifying the code:\n",
    "1. Change `d_k` and observe how attention patterns change\n",
    "2. Create your own sentences and visualize attention\n",
    "3. Try different mask patterns\n",
    "4. What happens without scaling (remove `/ sqrt(d_k)`)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
